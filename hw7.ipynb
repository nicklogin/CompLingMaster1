{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from lxml import html\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    if type(text) == str:\n",
    "        return [token.lemma_+'_'+token.pos_ for token in nlp(text) if token.pos_ not in ('AUX','PUNCT') \\\n",
    "                and token.text.lower() not in stops]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sents(sents, vectorizer, model):\n",
    "    vocab = sorted(vectorizer.vocabulary_, key=lambda x: vectorizer.vocabulary_[x])\n",
    "    word_vectors = np.array([model[word] if word in model else np.zeros(model.vectors.shape[1]) for word in vocab])\n",
    "    return vectorizer.transform(sents) * word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные соревнования Paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':[str(i) for i in texts_1], 'text_2':[str(i) for i in texts_2],\n",
    "                     'label':[int(i) for i in classes]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"task_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7227/7227 [00:45<00:00, 159.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7227/7227 [00:42<00:00, 171.32it/s]\n"
     ]
    }
   ],
   "source": [
    "data['text_1_norm'] = data['text_1'].progress_apply(process)\n",
    "data['text_2_norm'] = data['text_2'].progress_apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[полицейский_NOUN, разрешить_VERB, стрелять_VE...</td>\n",
       "      <td>[полиция_NOUN, мочь_VERB, разрешить_VERB, стре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0</td>\n",
       "      <td>[право_NOUN, полицейский_NOUN, проникновение_N...</td>\n",
       "      <td>[правило_NOUN, внесудебный_ADJ, проникновение_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0</td>\n",
       "      <td>[президент_NOUN, египет_PROPN, ввести_VERB, чр...</td>\n",
       "      <td>[власть_NOUN, египет_PROPN, угрожать_VERB, вве...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[вернуться_VERB, сирия_PROPN, россиянин_NOUN, ...</td>\n",
       "      <td>[самолёт_NOUN, мчс_PROPN, вывезти_VERB, россия...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0</td>\n",
       "      <td>[москва_PROPN, сирия_PROPN, вернуться_VERB, 2_...</td>\n",
       "      <td>[самолёт_NOUN, мчс_PROPN, вывезти_VERB, россия...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  Право полицейских на проникновение в жилище ре...   \n",
       "2  Президент Египта ввел чрезвычайное положение в...   \n",
       "3  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2  label  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...      0   \n",
       "1  Правила внесудебного проникновения полицейских...      0   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...      0   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...     -1   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...      0   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  [полицейский_NOUN, разрешить_VERB, стрелять_VE...   \n",
       "1  [право_NOUN, полицейский_NOUN, проникновение_N...   \n",
       "2  [президент_NOUN, египет_PROPN, ввести_VERB, чр...   \n",
       "3  [вернуться_VERB, сирия_PROPN, россиянин_NOUN, ...   \n",
       "4  [москва_PROPN, сирия_PROPN, вернуться_VERB, 2_...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  [полиция_NOUN, мочь_VERB, разрешить_VERB, стре...  \n",
       "1  [правило_NOUN, внесудебный_ADJ, проникновение_...  \n",
       "2  [власть_NOUN, египет_PROPN, угрожать_VERB, вве...  \n",
       "3  [самолёт_NOUN, мчс_PROPN, вывезти_VERB, россия...  \n",
       "4  [самолёт_NOUN, мчс_PROPN, вывезти_VERB, россия...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Готовая модель Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем модель http://vectors.nlpl.eu/repository/20/180.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_model = KeyedVectors.load_word2vec_format('gensim_model/model.bin',\n",
    "                                               binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['так_ADV', 'быть_VERB', 'мочь_VERB', 'год_NOUN', 'человек_NOUN']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in ready_model.vocab][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формат данных в модели - lemma_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные для обучения моделей gensim:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем данные проекта OpenCorpora (http://opencorpora.org/files/export/annot/annot.opcorpora.xml.zip) - морфологическую рзаметку использовать не будем, вместо этого анализируем их при помощи spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annot.opcorpora.xml','r',encoding='utf-8') as inp:\n",
    "    xml = inp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.findall('<source>(.*)?</source>', xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110304"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 110304/110304 [16:14<00:00, 113.19it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = pd.Series(sentences).progress_apply(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model1 = FastText(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение TfIdfVectorizer'а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_nothing = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.concat([data['text_1_norm'], data['text_2_norm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(preprocessor=do_nothing, tokenizer=do_nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\k1l77\\desktop\\182e~1\\term_p~1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function <lambda> at 0x000002C411F03598>,\n",
       "                tokenizer=<function <lambda> at 0x000002C411F03598>)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.fit(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуем тексты при помощи моделей, усреднив значения используя TfIdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_my = np.concatenate((vectorize_sents(data['text_1_norm'], tfidf_vec, my_model.wv),\n",
    "                         vectorize_sents(data['text_2_norm'], tfidf_vec, my_model.wv)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 200)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_my.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_ready = np.concatenate((vectorize_sents(data['text_1_norm'], tfidf_vec, ready_model),\n",
    "                         vectorize_sents(data['text_2_norm'], tfidf_vec, ready_model)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 600)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_ready.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним целевую переменную как y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать усреднённые при помоши TfIdf эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro(clf, X, y):\n",
    "    ## средний f1 по классам\n",
    "    y_pred = clf.predict(X)\n",
    "    return f1_score(y, y_pred, average='macro')\n",
    "\n",
    "def f1_micro(clf, X, y):\n",
    "    ## f1 относительно каждого объекта\n",
    "    y_pred = clf.predict(X)\n",
    "    return f1_score(y, y_pred, average='micro')\n",
    "    \n",
    "clf = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём кросс-валидацию для обученной нами модели Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_scores_macro = cross_val_score(clf, X_tfidf_my, y, scoring=f1_macro)\n",
    "my_model_scores_micro = cross_val_score(clf, X_tfidf_my, y, scoring=f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И для готовой модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_model_scores_macro = cross_val_score(clf, X_tfidf_ready, y, scoring=f1_macro)\n",
    "ready_model_scores_micro = cross_val_score(clf, X_tfidf_ready, y, scoring=f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним средний f1-score для обученной и готовой моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro F1 (Pretrained) 0.38130403589026246\n",
      "Micro F1 (Pretrained) 0.41303316151942837\n",
      "Macro F1 (Trained)    0.34647277494724743\n",
      "Micro F1 (Trained)    0.44665422331978927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Macro F1 (Pretrained) {np.mean(ready_model_scores_macro)}\n",
    "Micro F1 (Pretrained) {np.mean(ready_model_scores_micro)}\n",
    "Macro F1 (Trained)    {np.mean(my_model_scores_macro)}\n",
    "Micro F1 (Trained)    {np.mean(my_model_scores_micro)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Вывод</b>: Наша модель спрваляется лучше в целом, но хуже для классов по отдельности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которая получив данные и модели, выдаст датафрейм с нужными расстояниями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_cosine(a,b):\n",
    "    assert len(a) == len(b)\n",
    "    \n",
    "    length = len(a)\n",
    "    outp = np.zeros(length)\n",
    "    \n",
    "    for i in range(length):\n",
    "        outp[i] = cosine(a[i], b[i])\n",
    "    \n",
    "    return outp\n",
    "\n",
    "def cosine_dist(data, func):\n",
    "    return double_cosine(func(data['text_1_norm']),\n",
    "                 func(data['text_2_norm']))\n",
    "\n",
    "def get_dataset(data, wv_model1, wv_model2, ft_model,\n",
    "                svd_dim=200, nmf_dim=50):\n",
    "    do_nothing = lambda x: x\n",
    "    all_texts = pd.concat((data['text_1_norm'], data['text_2_norm']))\n",
    "    \n",
    "    print(\"Learning TF-IDF...\")\n",
    "    \n",
    "    vec = TfidfVectorizer(preprocessor=do_nothing, tokenizer=do_nothing)\n",
    "    all_texts = vec.fit_transform(all_texts)\n",
    "    \n",
    "    print(\"Learning SVD...\")\n",
    "    \n",
    "    svd = TruncatedSVD(svd_dim)\n",
    "    svd.fit(all_texts)\n",
    "    \n",
    "    print(\"Learning NMF...\")\n",
    "    \n",
    "    nmf = NMF(nmf_dim)\n",
    "    nmf.fit(all_texts)\n",
    "    \n",
    "    print(\"Calculating SVD distance...\")\n",
    "    svd_dist = cosine_dist(data, lambda x: svd.transform(vec.transform(x)))\n",
    "    print(\"Calculating NMF distance...\")\n",
    "    nmf_dist = cosine_dist(data, lambda x: nmf.transform(vec.transform(x)))\n",
    "    print(\"Calculating Word2Vec distance1...\")\n",
    "    wv1_dist = cosine_dist(data, lambda x: vectorize_sents(x, vec, wv_model1))\n",
    "    print(\"Calculating Word2Vec distance2...\")\n",
    "    wv2_dist = cosine_dist(data, lambda x: vectorize_sents(x, vec, wv_model2))\n",
    "    print(\"Calculating FastText distance...\")\n",
    "    ft_dist = cosine_dist(data, lambda x: vectorize_sents(x, vec, ft_model))\n",
    "    \n",
    "    y = data['label'].values\n",
    "    \n",
    "    return pd.DataFrame(np.vstack([svd_dist, nmf_dist, wv1_dist, wv2_dist, ft_dist, y]).transpose(),\n",
    "                       columns = ['svd_dist','nmf_dist', 'wv1_dist', 'wv2_dist', 'ft_dist', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим необходимый нам датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning TF-IDF...\n",
      "Learning SVD...\n",
      "Learning NMF...\n",
      "Calculating SVD distance...\n",
      "Calculating NMF distance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\k1l77\\desktop\\182e~1\\term_p~1\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Word2Vec distance1...\n",
      "Calculating Word2Vec distance2...\n",
      "Calculating FastText distance...\n"
     ]
    }
   ],
   "source": [
    "new_data = get_dataset(data, my_model.wv, ready_model, my_model1.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_dist</th>\n",
       "      <th>nmf_dist</th>\n",
       "      <th>wv1_dist</th>\n",
       "      <th>wv2_dist</th>\n",
       "      <th>ft_dist</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566086</td>\n",
       "      <td>0.972402</td>\n",
       "      <td>0.091724</td>\n",
       "      <td>0.361776</td>\n",
       "      <td>0.082695</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.201763</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.123796</td>\n",
       "      <td>0.179390</td>\n",
       "      <td>0.106179</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.263054</td>\n",
       "      <td>0.743416</td>\n",
       "      <td>0.048263</td>\n",
       "      <td>0.144536</td>\n",
       "      <td>0.144733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.367196</td>\n",
       "      <td>0.392257</td>\n",
       "      <td>0.126012</td>\n",
       "      <td>0.533401</td>\n",
       "      <td>0.290044</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197857</td>\n",
       "      <td>0.141857</td>\n",
       "      <td>0.029121</td>\n",
       "      <td>0.299286</td>\n",
       "      <td>0.315807</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   svd_dist  nmf_dist  wv1_dist  wv2_dist   ft_dist  label\n",
       "0  0.566086  0.972402  0.091724  0.361776  0.082695    0.0\n",
       "1  0.201763  0.001256  0.123796  0.179390  0.106179    0.0\n",
       "2  0.263054  0.743416  0.048263  0.144536  0.144733    0.0\n",
       "3  0.367196  0.392257  0.126012  0.533401  0.290044   -1.0\n",
       "4  0.197857  0.141857  0.029121  0.299286  0.315807    0.0"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 6)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7217, 6)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_clear = new_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём классификацию и оценим качество:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = new_data_clear.drop(['label'], axis=1), new_data_clear['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = cross_val_score(clf, X, y, scoring=f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56786704, 0.56371191, 0.6022176 , 0.48579349, 0.49410949])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5427399058978006"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Средння f1 уже выше чем при предыдущем методе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Вывод</b>: Использование расстояний между векторами текстов вместо самих векторов позволяет эффективнее решить задачу определения перефразирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем увеличить количество измерений для NMF и SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfmatrix = tfidf_vec.transform(all_texts)\n",
    "\n",
    "nmf = NMF(100).fit(tfidfmatrix)\n",
    "svd = TruncatedSVD(250).fit(tfidfmatrix)\n",
    "\n",
    "nmf_transform = lambda x: nmf.transform(tfidf_vec.transform(x))\n",
    "svd_transform = lambda x: svd.transform(tfidf_vec.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\k1l77\\desktop\\182e~1\\term_p~1\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "new_data['nmf_dist'] = cosine_dist(data, nmf_transform)\n",
    "new_data['svd_dist'] = cosine_dist(data, svd_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = new_data_clear.drop(['label'], axis=1), new_data_clear['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7217, 5)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = new_data_clear.drop(['label'], axis=1), new_data_clear['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = cross_val_score(clf, X, y, scoring=f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57479224, 0.56440443, 0.6042966 , 0.48371448, 0.4961885 ])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изменившуюся f-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5446792520199721"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Вывод</b>: Увеличив количество измерений в SVD и NMF, удалось увеличить качество классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
